name: VoxBridge Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements-bot.txt', 'requirements-test.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-bot.txt
          pip install -r requirements-test.txt

      - name: Run unit tests
        run: |
          pytest tests/unit -v --cov=. --cov-report=xml --cov-report=term-missing

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: unit-tests
          name: unit-tests-${{ matrix.python-version }}

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-bot.txt
          pip install -r requirements-test.txt

      - name: Run integration tests
        run: |
          pytest tests/integration -v --cov=. --cov-report=xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          flags: integration-tests
          name: integration-tests

  lint-and-type-check:
    name: Lint and Type Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install ruff mypy

      - name: Run ruff
        run: |
          ruff check .

      - name: Run mypy
        run: |
          mypy . --ignore-missing-imports
        continue-on-error: true  # Don't fail on type errors initially

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    # Only run E2E on pull requests to save resources
    if: github.event_name == 'pull_request'

    services:
      # Note: WhisperX requires GPU, so E2E tests may need self-hosted runner
      # For now, we'll skip WhisperX-dependent tests in CI

      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-bot.txt
          pip install -r requirements-test.txt

      - name: Run E2E tests (without WhisperX/Chatterbox)
        run: |
          # Skip tests requiring GPU services
          pytest tests/e2e -v \
            -m "not whisperx and not chatterbox and not n8n" \
            --ignore=tests/e2e/test_n8n_real.py
        continue-on-error: true  # E2E may not be fully functional in CI

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, lint-and-type-check]
    if: always()

    steps:
      - name: Check test results
        run: |
          if [ "${{ needs.unit-tests.result }}" != "success" ] || \
             [ "${{ needs.integration-tests.result }}" != "success" ]; then
            echo "❌ Some tests failed"
            exit 1
          fi
          echo "✅ All tests passed"
