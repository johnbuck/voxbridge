# VoxBridge Environment Configuration
# Copy this file to .env and fill in your values
# WARNING: Never commit .env files - they contain secrets!

# ==============================================================================
# DISCORD CONFIGURATION
# ==============================================================================

# Discord Bot Token (REQUIRED)
# Get from: https://discord.com/developers/applications
# Example format: YOUR_BOT_ID.YOUR_TIMESTAMP.YOUR_SIGNATURE_HERE
DISCORD_TOKEN=your_discord_bot_token_here

# Discord Bot Mode (Phase 6.4.1 Batch 2a: Graceful Deprecation)
# Set to 'true' to use legacy Discord bot voice handlers (rollback option)
# Set to 'false' to use new plugin-based Discord bot (recommended)
# WARNING: Legacy mode is deprecated and will be removed in VoxBridge 3.0
USE_LEGACY_DISCORD_BOT=false

# ==============================================================================
# DATABASE CONFIGURATION (VoxBridge 2.0)
# ==============================================================================

# PostgreSQL Database Connection
# Used for storing agents, sessions, and conversation history
POSTGRES_USER=voxbridge
POSTGRES_PASSWORD=voxbridge_dev_password
POSTGRES_DB=voxbridge

# Database URL (auto-constructed from above, or override)
# Format: postgresql+asyncpg://user:password@host:port/database
# Default: postgresql+asyncpg://voxbridge:voxbridge_dev_password@postgres:5432/voxbridge
# DATABASE_URL=postgresql+asyncpg://voxbridge:voxbridge_dev_password@postgres:5432/voxbridge

# ==============================================================================
# PLUGIN SYSTEM SECURITY (VoxBridge 2.0 Phase 6)
# ==============================================================================

# Plugin Encryption Key (REQUIRED for production)
# 32-byte Fernet key for encrypting sensitive plugin fields (bot tokens, API keys)
# Generate with: python -c 'from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())'
# WARNING: Keep this secret! Changing this key will invalidate all existing encrypted plugin configs
# PLUGIN_ENCRYPTION_KEY=your_generated_encryption_key_here

# ==============================================================================
# LLM PROVIDER CONFIGURATION (VoxBridge 2.0)
# ==============================================================================

# OpenRouter API Key (Optional - for OpenRouter LLM provider)
# Get from: https://openrouter.ai/keys
# Used when agent is configured with OpenRouter provider
# OPENROUTER_API_KEY=your_openrouter_api_key_here

# Local LLM Base URL (Optional - for Local LLM provider)
# OpenAI-compatible API endpoint (e.g., Ollama, LM Studio, vLLM)
# Example: http://localhost:11434/v1 (Ollama)
# LOCAL_LLM_BASE_URL=http://localhost:11434/v1

# ==============================================================================
# MEMORY SYSTEM CONFIGURATION (VoxBridge 2.0 Phase 2)
# ==============================================================================

# Embedding Provider Selection
# Options: local, azure
#   - local: Sentence-transformers (free, 768 dims, self-hosted) [DEFAULT - works out of the box]
#   - azure: Azure OpenAI embeddings (best quality, 3072 dims, $1/month, requires API key)
#
# Configuration Priority (matches LLM provider pattern):
# 1. Database (system_settings table, configurable via Settings UI) - HIGHEST PRIORITY
# 2. Environment variables (below) - Used if database not configured
# 3. Hardcoded defaults (local embeddings) - Fallback
#
# Recommended: Configure via Settings UI (/settings/embeddings) for persistence across container restarts
# Note: Settings UI will be restricted to admin-only access in a future phase
EMBEDDING_PROVIDER=local

# Azure OpenAI Embeddings (when EMBEDDING_PROVIDER=azure)
# Get Azure API key from: https://portal.azure.com
# Provides state-of-the-art quality with text-embedding-3-large (3072 dimensions)
# Cost: ~$0.13 per 1M tokens (~$1/month for 30K conversations)
AZURE_EMBEDDING_API_KEY=your_azure_embedding_api_key_here
AZURE_EMBEDDING_ENDPOINT=https://your-resource.openai.azure.com
AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large
AZURE_EMBEDDING_API_VERSION=2024-12-01-preview
AZURE_EMBEDDING_DIMS=3072

# Local Open-Source Embeddings (when EMBEDDING_PROVIDER=local)
# Free, self-hosted alternative using sentence-transformers
# Recommended models:
#   - sentence-transformers/all-mpnet-base-v2 (768 dims, good quality, 420MB) [DEFAULT]
#   - sentence-transformers/all-MiniLM-L6-v2 (384 dims, fast, 80MB)
#   - BAAI/bge-large-en-v1.5 (1024 dims, better quality, 1.34GB)
# Cost: $0/month, but slower inference (~200-500ms vs 50ms Azure)
LOCAL_EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
LOCAL_EMBEDDING_DIMS=768

# Memory Service - LLM Relevance Filtering (VoxBridge 2.0 Phase 2)
# Model used to determine if conversations contain facts worth extracting
# This model analyzes user/AI conversations and decides whether to extract memory facts
# Options: Any Ollama model (gemma3n:latest, llama3.2:3b, etc.)
# Recommended: Small, fast models (3B-7B parameters) for quick yes/no decisions
LOCAL_LLM_RELEVANCE_MODEL=gemma3n:latest

# Model Cache Location (VoxBridge 2.0 Phase 2)
# HuggingFace cache directory for downloaded embedding models
# Models are cached in Docker volume: voxbridge-models
# Default: /home/appuser/.cache/huggingface (set in docker-compose.yml)
# HF_HOME=/home/appuser/.cache/huggingface

# ==============================================================================
# WHISPERX STT CONFIGURATION
# ==============================================================================

# WhisperX Model Selection
# Options: tiny, base, small, medium, large-v2
# tiny    - Fastest, lowest accuracy (~85%), ~1GB RAM
# base    - Fast, good accuracy (~95%), ~1GB RAM [RECOMMENDED FOR TESTING]
# small   - Medium speed, better accuracy (~97%), ~2GB RAM [RECOMMENDED FOR PRODUCTION]
# medium  - Slower, high accuracy (~98%), ~5GB RAM
# large-v2 - Slowest, highest accuracy (~99%), ~10GB RAM
WHISPERX_MODEL=small

# WhisperX Device Selection
# auto - Automatically detect GPU, fallback to CPU
# cuda - Force GPU usage (requires NVIDIA GPU + Container Toolkit)
# cpu  - Force CPU usage (slower but works everywhere)
WHISPERX_DEVICE=auto

# WhisperX Compute Type
# float16 - GPU recommended, better accuracy
# int8    - CPU recommended, faster on CPU
# Default: auto-selected based on device
WHISPERX_COMPUTE_TYPE=float16

# WhisperX Batch Size
# Higher = faster but more VRAM/RAM
# Recommended: 16 for GPU, 8 for CPU
WHISPERX_BATCH_SIZE=16

# WhisperX Server Configuration
# WebSocket URL for WhisperX server (internal Docker network)
WHISPER_SERVER_URL=ws://whisperx:4901
WHISPER_SERVER_PORT=4901

# WhisperX VAD Configuration (TTS Echo Prevention)
# Voice Activity Detection tuning for filtering TTS echoes
# VAD Onset - Speech start detection threshold (0.0-1.0)
# Higher = less sensitive, filters more aggressively
# Lower = more sensitive, catches more speech (including echoes)
# Recommended: 0.600 for TTS echo prevention (default: 0.500)
WHISPERX_VAD_ONSET=0.600

# VAD Offset - Speech end detection threshold (0.0-1.0)
# Higher = less sensitive, ends speech detection earlier
# Lower = more sensitive, extends speech segments longer
# Recommended: 0.450 for TTS echo prevention (default: 0.363)
WHISPERX_VAD_OFFSET=0.450

# ==============================================================================
# SPEAKER MANAGEMENT
# ==============================================================================

# Silence Detection Threshold (milliseconds)
# How long to wait for silence before finalizing transcript
# Lower = faster response, might cut off sentences
# Higher = more complete sentences, longer wait
# Recommended: 400-600ms (reduced for streaming optimization)
SILENCE_THRESHOLD_MS=400

# Maximum Utterance Time (milliseconds)
# Safety timeout per speaking turn - force-finalize if user speaks continuously beyond this limit
# Resets after each finalization to support multi-turn conversations
# Set to 0 for unlimited utterance duration
# Can be overridden per-agent in the database/UI
# Recommended: 120000ms (2 minutes per speaking turn)
# Note: Replaces MAX_SPEAKING_TIME_MS (deprecated)
MAX_UTTERANCE_TIME_MS=120000

# VAD Enhancement (Option B: TTS Echo Prevention)
# Minimum Speech Duration - Require sustained speech before transcription
# Filters out brief echoes, clicks, and noise that don't meet duration threshold
# Recommended: 500ms (balances echo filtering with interrupt detection)
# Lower = faster interrupt response but more false positives
# Higher = fewer false positives but delayed interrupt detection
MIN_SPEECH_DURATION_MS=500

# Speech Energy Threshold - Minimum amplitude for speech detection
# Audio chunks below this energy level are filtered as silence/noise
# Recommended: 300 (int16 PCM amplitude average)
# Lower = more sensitive but may accept echoes
# Higher = stricter filtering but may miss quiet speech
SPEECH_ENERGY_THRESHOLD=300

# ==============================================================================
# STREAMING CONFIGURATION (VoxBridge 2.0)
# ==============================================================================

# Enable LLM Response Streaming
# When enabled, LLM responses are processed chunk-by-chunk for lower latency
# Reduces time-to-first-audio by 60-70% (from ~6.8s to ~2-3s)
# Can also be configured via Frontend Settings UI
STREAMING_ENABLED=true

# Chunking Strategy
# How to split LLM responses for TTS processing
# Options: sentence, paragraph, word, fixed
#   - sentence: Split on sentence boundaries (periods, questions, exclamations)
#   - paragraph: Split on paragraph boundaries (double newlines)
#   - word: Split on word boundaries (for very low latency)
#   - fixed: Split at fixed character intervals
# Recommended: sentence (best balance of naturalness and latency)
STREAMING_CHUNKING_STRATEGY=sentence

# Minimum Chunk Length (characters)
# Chunks shorter than this will be buffered with the next chunk
# Prevents synthesizing very short phrases like "Hi." or "Oh."
# Recommended: 10-20 characters
# Range: 5-200 characters
STREAMING_MIN_CHUNK_LENGTH=10

# Maximum Concurrent TTS Requests
# How many TTS synthesis requests can run in parallel
# Higher = faster but more GPU memory usage
# Lower = safer but chunks wait in queue
# Recommended: 3-4 for typical responses
# Note: This works alongside Chatterbox's native parallelization
STREAMING_MAX_CONCURRENT_TTS=3

# TTS Error Strategy
# How to handle failures when synthesizing individual chunks
# Options: skip, retry, fallback
#   - skip: Continue with next chunk (fastest, partial audio)
#   - retry: Retry up to 2 times with backoff (most reliable)
#   - fallback: Synthesize remaining text as single chunk (safe fallback)
# Can be overridden via Frontend Settings UI
STREAMING_ERROR_STRATEGY=retry

# User Interruption Strategy
# How to handle when user starts speaking while AI is responding
# Options: immediate, graceful, drain
#   - immediate: Cancel all queued TTS and stop playback (most responsive)
#   - graceful: Finish current chunk, cancel queue (better UX)
#   - drain: Process 1-2 more chunks from queue, then stop (completes thought)
# Can be overridden via Frontend Settings UI
STREAMING_INTERRUPTION_STRATEGY=graceful

# ==============================================================================
# CHATTERBOX TTS CONFIGURATION
# ==============================================================================

# Chatterbox TTS Server URL (REQUIRED)
# Internal Docker network: http://chatterbox-tts-api-uv-blackwell:4123
# External/localhost: http://localhost:4123
# Example: http://chatterbox:4800
CHATTERBOX_URL=http://chatterbox-tts-api-uv-blackwell:4123

# Chatterbox Voice ID (REQUIRED)
# The voice to use for TTS responses
# Example: "auren_voice" or specific voice model ID
CHATTERBOX_VOICE_ID=your_voice_id_here

# ==============================================================================
# N8N INTEGRATION
# ==============================================================================

# n8n Webhook URL (REQUIRED)
# Production webhook for AI responses
# Example: https://your-n8n-instance.com/webhook/voxbridge-production
N8N_WEBHOOK_URL=your_n8n_webhook_url_here

# n8n Test Webhook URL (Optional)
# Used when N8N_TEST_MODE=true
# Example: https://your-n8n-instance.com/webhook/voxbridge-test
N8N_WEBHOOK_TEST_URL=your_n8n_test_webhook_url_here

# n8n Test Mode
# true  - Use N8N_WEBHOOK_TEST_URL instead of N8N_WEBHOOK_URL
# false - Use production N8N_WEBHOOK_URL
N8N_TEST_MODE=false

# ==============================================================================
# STREAMING CONFIGURATION
# ==============================================================================

# Enable Streaming Responses
# true  - Stream AI responses sentence-by-sentence (90%+ latency reduction)
# false - Wait for complete response before speaking
USE_STREAMING=true

# Clause Splitting
# true  - Split on clauses (commas, semicolons) for even lower latency
# false - Only split on sentences (periods, question marks, exclamation marks)
USE_CLAUSE_SPLITTING=true

# Parallel TTS Generation
# true  - Generate TTS for multiple sentences in parallel
# false - Generate TTS sequentially (more stable)
# WARNING: Parallel mode is experimental and may cause issues
USE_PARALLEL_TTS=false

# Progressive TTS Playback
# true  - Start playing TTS as soon as first sentence is ready
# false - Buffer complete response before playing
USE_PROGRESSIVE_TTS_PLAYBACK=false

# ==============================================================================
# THINKING INDICATORS
# ==============================================================================

# Enable Thinking Indicator Sound
# true  - Play looping audio while waiting for AI response
# false - Silent waiting period
USE_THINKING_INDICATORS=true

# Thinking Indicator Probability
# Percentage (0.0-1.0) of the time to play thinking indicator
# 0.8 = 80% of requests will have thinking sound
# Recommended: 0.8 (balance between UX and annoyance)
THINKING_INDICATOR_PROBABILITY=0.8

# ==============================================================================
# ADVANCED STREAMING SETTINGS
# ==============================================================================

# Minimum Clause Length
# Minimum characters before splitting on clauses
# Prevents very short clause splits like "Hi, there"
MIN_CLAUSE_LENGTH=10

# Minimum Sentence Length
# Minimum characters before considering it a sentence
# Prevents very short fragments
MIN_SENTENCE_LENGTH=3

# Discord Audio Queue Size
# Maximum number of audio chunks to buffer
# Higher = more memory but smoother playback
DISCORD_AUDIO_QUEUE_SIZE=100

# ==============================================================================
# SERVER CONFIGURATION
# ==============================================================================

# VoxBridge Discord Bot Server Port
# FastAPI server port for HTTP API and WebSocket
PORT=4900

# Frontend Port (configured in docker-compose.yml)
# Default: 4903
# Access dashboard at http://localhost:4903
FRONTEND_PORT=4903

# ==============================================================================
# LOGGING CONFIGURATION (Tiered System)
# ==============================================================================

# Global Log Level
# Controls default verbosity for all modules
# Options: TRACE, DEBUG, INFO, WARN, ERROR
# TRACE - Ultra-verbose debugging (raw data, every iteration)
# DEBUG - Detailed debugging (checkpoints, state changes)
# INFO  - Standard operational messages (connections, completions) [RECOMMENDED]
# WARN  - Warnings (recoverable errors, fallbacks)
# ERROR - Errors only (exceptions, failures)
LOG_LEVEL=INFO

# Per-Module Log Level Overrides
# Override global log level for specific modules
# Uncomment and adjust as needed for troubleshooting

# Voice/WebRTC logging (browser audio streaming, WebM decoding)
# LOG_LEVEL_VOICE=DEBUG
# LOG_LEVEL_WEBRTC=DEBUG

# STT logging (WhisperX transcription)
# LOG_LEVEL_STT=DEBUG

# LLM logging (OpenRouter/Local LLM generation)
# LOG_LEVEL_LLM=DEBUG

# TTS logging (Chatterbox synthesis)
# LOG_LEVEL_TTS=DEBUG

# Conversation logging (session management, context caching)
# LOG_LEVEL_CONVERSATION=DEBUG

# Discord plugin logging (Discord bot events, voice channels)
# LOG_LEVEL_DISCORD=DEBUG

# ==============================================================================
# GPU CONFIGURATION (Advanced)
# ==============================================================================

# GPU Device ID
# Which GPU to use (0 = first GPU, 1 = second GPU)
# Configured in docker-compose.yml deploy.resources.reservations.devices
# WhisperX uses GPU 1 (RTX 5060 Ti) by default

# CUDA Visible Devices (Optional)
# Control which GPUs are visible to the container
# Example: "0,1" for first two GPUs, "1" for second GPU only
# CUDA_VISIBLE_DEVICES=1
