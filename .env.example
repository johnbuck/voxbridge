# VoxBridge Environment Configuration
# Copy this file to .env and fill in your values
# WARNING: Never commit .env files - they contain secrets!

# ==============================================================================
# DISCORD CONFIGURATION
# ==============================================================================

# Discord Bot Token (REQUIRED)
# Get from: https://discord.com/developers/applications
# Example: MTQyOTk4MjA0MTM0ODM3ODc3Ng.GxxxXx.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
DISCORD_TOKEN=your_discord_bot_token_here

# ==============================================================================
# DATABASE CONFIGURATION (VoxBridge 2.0)
# ==============================================================================

# PostgreSQL Database Connection
# Used for storing agents, sessions, and conversation history
POSTGRES_USER=voxbridge
POSTGRES_PASSWORD=voxbridge_dev_password
POSTGRES_DB=voxbridge

# Database URL (auto-constructed from above, or override)
# Format: postgresql+asyncpg://user:password@host:port/database
# Default: postgresql+asyncpg://voxbridge:voxbridge_dev_password@postgres:5432/voxbridge
# DATABASE_URL=postgresql+asyncpg://voxbridge:voxbridge_dev_password@postgres:5432/voxbridge

# ==============================================================================
# LLM PROVIDER CONFIGURATION (VoxBridge 2.0)
# ==============================================================================

# OpenRouter API Key (Optional - for OpenRouter LLM provider)
# Get from: https://openrouter.ai/keys
# Used when agent is configured with OpenRouter provider
# OPENROUTER_API_KEY=your_openrouter_api_key_here

# Local LLM Base URL (Optional - for Local LLM provider)
# OpenAI-compatible API endpoint (e.g., Ollama, LM Studio, vLLM)
# Example: http://localhost:11434/v1 (Ollama)
# LOCAL_LLM_BASE_URL=http://localhost:11434/v1

# ==============================================================================
# WHISPERX STT CONFIGURATION
# ==============================================================================

# WhisperX Model Selection
# Options: tiny, base, small, medium, large-v2
# tiny    - Fastest, lowest accuracy (~85%), ~1GB RAM
# base    - Fast, good accuracy (~95%), ~1GB RAM [RECOMMENDED FOR TESTING]
# small   - Medium speed, better accuracy (~97%), ~2GB RAM [RECOMMENDED FOR PRODUCTION]
# medium  - Slower, high accuracy (~98%), ~5GB RAM
# large-v2 - Slowest, highest accuracy (~99%), ~10GB RAM
WHISPERX_MODEL=small

# WhisperX Device Selection
# auto - Automatically detect GPU, fallback to CPU
# cuda - Force GPU usage (requires NVIDIA GPU + Container Toolkit)
# cpu  - Force CPU usage (slower but works everywhere)
WHISPERX_DEVICE=auto

# WhisperX Compute Type
# float16 - GPU recommended, better accuracy
# int8    - CPU recommended, faster on CPU
# Default: auto-selected based on device
WHISPERX_COMPUTE_TYPE=float16

# WhisperX Batch Size
# Higher = faster but more VRAM/RAM
# Recommended: 16 for GPU, 8 for CPU
WHISPERX_BATCH_SIZE=16

# WhisperX Server Configuration
# WebSocket URL for WhisperX server (internal Docker network)
WHISPER_SERVER_URL=ws://whisperx:4901
WHISPER_SERVER_PORT=4901

# ==============================================================================
# SPEAKER MANAGEMENT
# ==============================================================================

# Silence Detection Threshold (milliseconds)
# How long to wait for silence before finalizing transcript
# Lower = faster response, might cut off sentences
# Higher = more complete sentences, longer wait
# Recommended: 600-800ms
SILENCE_THRESHOLD_MS=600

# Maximum Speaking Time (milliseconds)
# Safety timeout to prevent one person monopolizing the bot
# Recommended: 45000ms (45 seconds)
MAX_SPEAKING_TIME_MS=45000

# ==============================================================================
# CHATTERBOX TTS CONFIGURATION
# ==============================================================================

# Chatterbox TTS Server URL (REQUIRED)
# Internal Docker network: http://chatterbox-tts-api-uv-blackwell:4123
# External/localhost: http://localhost:4123
# Example: http://chatterbox:4800
CHATTERBOX_URL=http://chatterbox-tts-api-uv-blackwell:4123

# Chatterbox Voice ID (REQUIRED)
# The voice to use for TTS responses
# Example: "auren_voice" or specific voice model ID
CHATTERBOX_VOICE_ID=your_voice_id_here

# ==============================================================================
# N8N INTEGRATION
# ==============================================================================

# n8n Webhook URL (REQUIRED)
# Production webhook for AI responses
# Example: https://your-n8n-instance.com/webhook/voxbridge-production
N8N_WEBHOOK_URL=your_n8n_webhook_url_here

# n8n Test Webhook URL (Optional)
# Used when N8N_TEST_MODE=true
# Example: https://your-n8n-instance.com/webhook/voxbridge-test
N8N_WEBHOOK_TEST_URL=your_n8n_test_webhook_url_here

# n8n Test Mode
# true  - Use N8N_WEBHOOK_TEST_URL instead of N8N_WEBHOOK_URL
# false - Use production N8N_WEBHOOK_URL
N8N_TEST_MODE=false

# ==============================================================================
# STREAMING CONFIGURATION
# ==============================================================================

# Enable Streaming Responses
# true  - Stream AI responses sentence-by-sentence (90%+ latency reduction)
# false - Wait for complete response before speaking
USE_STREAMING=true

# Clause Splitting
# true  - Split on clauses (commas, semicolons) for even lower latency
# false - Only split on sentences (periods, question marks, exclamation marks)
USE_CLAUSE_SPLITTING=true

# Parallel TTS Generation
# true  - Generate TTS for multiple sentences in parallel
# false - Generate TTS sequentially (more stable)
# WARNING: Parallel mode is experimental and may cause issues
USE_PARALLEL_TTS=false

# Progressive TTS Playback
# true  - Start playing TTS as soon as first sentence is ready
# false - Buffer complete response before playing
USE_PROGRESSIVE_TTS_PLAYBACK=false

# ==============================================================================
# THINKING INDICATORS
# ==============================================================================

# Enable Thinking Indicator Sound
# true  - Play looping audio while waiting for AI response
# false - Silent waiting period
USE_THINKING_INDICATORS=true

# Thinking Indicator Probability
# Percentage (0.0-1.0) of the time to play thinking indicator
# 0.8 = 80% of requests will have thinking sound
# Recommended: 0.8 (balance between UX and annoyance)
THINKING_INDICATOR_PROBABILITY=0.8

# ==============================================================================
# ADVANCED STREAMING SETTINGS
# ==============================================================================

# Minimum Clause Length
# Minimum characters before splitting on clauses
# Prevents very short clause splits like "Hi, there"
MIN_CLAUSE_LENGTH=10

# Minimum Sentence Length
# Minimum characters before considering it a sentence
# Prevents very short fragments
MIN_SENTENCE_LENGTH=3

# Discord Audio Queue Size
# Maximum number of audio chunks to buffer
# Higher = more memory but smoother playback
DISCORD_AUDIO_QUEUE_SIZE=100

# ==============================================================================
# SERVER CONFIGURATION
# ==============================================================================

# VoxBridge Discord Bot Server Port
# FastAPI server port for HTTP API and WebSocket
PORT=4900

# Frontend Port (configured in docker-compose.yml)
# Default: 4903
# Access dashboard at http://localhost:4903
FRONTEND_PORT=4903

# ==============================================================================
# GPU CONFIGURATION (Advanced)
# ==============================================================================

# GPU Device ID
# Which GPU to use (0 = first GPU, 1 = second GPU)
# Configured in docker-compose.yml deploy.resources.reservations.devices
# WhisperX uses GPU 1 (RTX 5060 Ti) by default

# CUDA Visible Devices (Optional)
# Control which GPUs are visible to the container
# Example: "0,1" for first two GPUs, "1" for second GPU only
# CUDA_VISIBLE_DEVICES=1
