#!/usr/bin/env python3
"""
============================================================
WebRTC Voice Handler
Handles browser audio streaming via WebSocket:
- Receive Opus/WebM audio chunks from browser
- Decode to PCM for WhisperX
- Stream transcriptions back to browser
- Route final transcript to LLM
- Stream AI response chunks to browser
============================================================
"""

import asyncio
import logging
import os
import time
from io import BytesIO
from typing import Optional
from uuid import UUID

from fastapi import WebSocket, WebSocketDisconnect
import opuslib
import httpx

from src.whisper_client import WhisperClient
from src.services.session_service import SessionService
from src.services.agent_service import AgentService
from src.llm.factory import LLMProviderFactory
from src.llm.types import LLMMessage, LLMRequest, LLMError

logger = logging.getLogger(__name__)


class WebRTCVoiceHandler:
    """
    Handles WebRTC voice streaming from browser

    Architecture:
    1. Browser ‚Üí WebSocket: Binary Opus chunks (100ms each)
    2. Opus ‚Üí PCM: Decode with opuslib (16kHz mono)
    3. PCM ‚Üí WhisperX: Stream for transcription
    4. WhisperX ‚Üí Browser: Partial/final transcripts
    5. LLM ‚Üí Browser: Stream AI response chunks
    """

    def __init__(self, websocket: WebSocket, user_id: str, session_id: UUID):
        """
        Initialize WebRTC voice handler

        Args:
            websocket: FastAPI WebSocket connection
            user_id: User identifier (browser session ID)
            session_id: Active session ID for this conversation
        """
        self.websocket = websocket
        self.user_id = user_id
        self.session_id = session_id
        self.is_active = True

        # WhisperX client for transcription
        self.whisper_client: Optional[WhisperClient] = None

        # Audio processing
        self.audio_buffer = BytesIO()
        self.opus_decoder = opuslib.Decoder(16000, 1)  # 16kHz mono

        # VAD settings (reuse from Discord configuration)
        self.silence_threshold_ms = int(os.getenv('SILENCE_THRESHOLD_MS', '600'))
        self.last_audio_time: Optional[float] = None
        self.silence_task: Optional[asyncio.Task] = None

        # Transcription state
        self.current_transcript = ""
        self.is_finalizing = False

        # Timing metrics
        self.t_start = time.time()
        self.t_first_audio = None
        self.t_first_transcript = None

        logger.info(f"üéôÔ∏è WebRTC handler initialized for user={user_id}, session={session_id}")
        logger.info(f"   Silence threshold: {self.silence_threshold_ms}ms")

    async def start(self):
        """
        Start handling audio stream

        Main loop:
        1. Accept WebSocket connection
        2. Connect to WhisperX
        3. Receive audio chunks
        4. Process transcripts
        5. Handle disconnection
        """
        try:
            # Validate session exists and user owns it
            session = await SessionService.get_session(self.session_id)
            if not session:
                await self._send_error("Session not found")
                return

            if session.user_id != self.user_id:
                await self._send_error("Session does not belong to user")
                return

            logger.info(f"‚úÖ Session validated: {session.title} (agent: {session.agent_id})")

            # Connect to WhisperX
            await self._connect_whisperx()

            # Start audio streaming loop
            await self._audio_loop()

        except WebSocketDisconnect:
            logger.info(f"üîå WebSocket disconnected for user {self.user_id}")
        except Exception as e:
            logger.error(f"‚ùå Error in WebRTC handler: {e}", exc_info=True)
            await self._send_error(f"Server error: {str(e)}")
        finally:
            await self._cleanup()

    async def _connect_whisperx(self):
        """Connect to WhisperX server and set up callbacks"""
        try:
            logger.info(f"üîå Connecting to WhisperX for user {self.user_id}")

            self.whisper_client = WhisperClient()

            # Set partial transcript callback
            async def on_partial(text: str):
                if self.t_first_transcript is None:
                    self.t_first_transcript = time.time()
                    latency_s = self.t_first_transcript - self.t_start
                    logger.info(f"‚è±Ô∏è LATENCY [connection ‚Üí first transcript]: {latency_s:.3f}s")

                self.current_transcript = text
                await self._send_partial_transcript(text)

            # Set final transcript callback
            async def on_final(text: str):
                logger.info(f"‚úÖ Final transcript from WhisperX: \"{text}\"")
                # Don't process here - wait for finalize() call

            self.whisper_client.on_partial_callback = on_partial
            self.whisper_client.on_final_callback = on_final

            # Connect
            await self.whisper_client.connect(self.user_id)
            logger.info(f"‚úÖ Connected to WhisperX")

        except Exception as e:
            logger.error(f"‚ùå Failed to connect to WhisperX: {e}")
            raise

    async def _audio_loop(self):
        """
        Main audio processing loop

        Receives binary audio chunks from browser and processes them:
        1. Receive Opus/WebM binary data
        2. Decode to PCM
        3. Stream to WhisperX
        4. Monitor for silence
        """
        logger.info(f"üéôÔ∏è Starting audio stream loop")

        # Start silence monitoring
        self.silence_task = asyncio.create_task(self._monitor_silence())

        try:
            while self.is_active:
                # Receive binary audio chunk from browser
                audio_data = await self.websocket.receive_bytes()

                if self.t_first_audio is None:
                    self.t_first_audio = time.time()
                    logger.info(f"üé§ Received first audio chunk ({len(audio_data)} bytes)")

                # Update silence detection
                self.last_audio_time = time.time()

                # Decode Opus to PCM
                try:
                    # Browser sends Opus frames (20ms each)
                    # opuslib expects frame_size parameter matching encoding
                    # For 16kHz: 20ms = 320 samples
                    pcm_data = self.opus_decoder.decode(audio_data, frame_size=320)

                    # Stream PCM to WhisperX
                    if self.whisper_client and self.whisper_client.is_connected:
                        await self.whisper_client.send_audio(pcm_data)

                except opuslib.OpusError as e:
                    logger.warning(f"‚ö†Ô∏è Opus decode error: {e}")
                    # Continue processing - may be partial frame

        except WebSocketDisconnect:
            logger.info(f"üîå Browser disconnected")
        except Exception as e:
            logger.error(f"‚ùå Error in audio loop: {e}", exc_info=True)
        finally:
            # Cancel silence monitoring
            if self.silence_task:
                self.silence_task.cancel()

    async def _monitor_silence(self):
        """
        Monitor for silence and trigger finalization

        Continuously checks if audio has stopped for silence_threshold_ms.
        When silence detected, finalizes transcription and calls LLM.
        """
        try:
            while self.is_active:
                await asyncio.sleep(0.1)  # Check every 100ms

                if self.last_audio_time:
                    silence_duration_ms = (time.time() - self.last_audio_time) * 1000

                    if silence_duration_ms >= self.silence_threshold_ms:
                        if not self.is_finalizing:
                            logger.info(f"ü§´ Silence detected ({int(silence_duration_ms)}ms) - finalizing")
                            await self._finalize_transcription()
                        break

        except asyncio.CancelledError:
            logger.debug("üõë Silence monitor cancelled")

    async def _finalize_transcription(self):
        """
        Finalize transcription and route to LLM

        Steps:
        1. Request final transcript from WhisperX
        2. Send final_transcript event to browser
        3. Save user message to database
        4. Route to LLM (based on agent config)
        5. Stream AI response back to browser
        6. Save AI message to database
        """
        if self.is_finalizing:
            return

        self.is_finalizing = True

        try:
            # Get final transcript from WhisperX
            if not self.whisper_client:
                logger.warning("‚ö†Ô∏è No WhisperX client for finalization")
                return

            transcript = await self.whisper_client.finalize()

            if not transcript or not transcript.strip():
                logger.info("üìù Empty transcript - skipping LLM processing")
                return

            logger.info(f"üìù Final transcript: \"{transcript}\"")

            # Send final transcript to browser
            await self._send_final_transcript(transcript)

            # Load session to get agent
            session = await SessionService.get_session(self.session_id)
            if not session:
                logger.error("‚ùå Session not found during finalization")
                return

            # Save user message to database
            await SessionService.add_message(
                session_id=self.session_id,
                role="user",
                content=transcript
            )
            logger.info(f"üíæ Saved user message to database")

            # Load agent configuration
            agent = await AgentService.get_agent(session.agent_id)
            if not agent:
                logger.error(f"‚ùå Agent not found: {session.agent_id}")
                return

            logger.info(f"ü§ñ Using agent: {agent.name} (provider: {agent.llm_provider}, model: {agent.llm_model})")

            # Route to LLM
            await self._handle_llm_response(transcript, agent)

        except Exception as e:
            logger.error(f"‚ùå Error finalizing transcription: {e}", exc_info=True)
            await self._send_error(f"Error processing transcript: {str(e)}")

    async def _handle_llm_response(self, transcript: str, agent):
        """
        Handle LLM response generation and streaming

        Args:
            transcript: User's transcribed text
            agent: Agent model instance with LLM configuration
        """
        try:
            t_llm_start = time.time()

            # Create LLM provider from agent configuration
            provider, model = LLMProviderFactory.create_from_agent_config(
                llm_provider=agent.llm_provider,
                llm_model=agent.llm_model
            )

            # Build conversation messages
            # TODO: Load conversation history from database
            messages = [
                LLMMessage(role="system", content=agent.system_prompt),
                LLMMessage(role="user", content=transcript)
            ]

            # Build LLM request
            request = LLMRequest(
                messages=messages,
                temperature=agent.temperature,
                model=model,
                max_tokens=None
            )

            logger.info(f"üì§ Sending to LLM ({agent.llm_provider}/{model}): \"{transcript}\"")

            # Stream response
            full_response = ""
            first_chunk_received = False

            async for chunk in provider.generate_stream(request):
                # Track first chunk latency
                if not first_chunk_received:
                    t_first_chunk = time.time()
                    latency_s = t_first_chunk - t_llm_start
                    logger.info(f"‚è±Ô∏è LATENCY [LLM first chunk]: {latency_s:.3f}s")
                    first_chunk_received = True

                # Accumulate response (chunks are plain strings)
                full_response += chunk

                # Stream chunk to browser
                await self._send_ai_response_chunk(chunk)

            # Send completion event
            await self._send_ai_response_complete(full_response)

            # Record latency
            t_llm_complete = time.time()
            latency_s = t_llm_complete - t_llm_start
            logger.info(f"‚è±Ô∏è LATENCY [total LLM generation]: {latency_s:.3f}s")

            # Save AI message to database
            await SessionService.add_message(
                session_id=self.session_id,
                role="assistant",
                content=full_response
            )
            logger.info(f"üíæ Saved AI message to database")

            # Generate and stream TTS audio to browser
            await self._handle_tts_response(full_response)

            # Close provider
            await provider.close()

        except LLMError as e:
            logger.error(f"‚ùå LLM Error: {e}", exc_info=True)
            await self._send_error(f"AI error: {str(e)}")
        except Exception as e:
            logger.error(f"‚ùå Error handling LLM response: {e}", exc_info=True)
            await self._send_error(f"Error generating AI response: {str(e)}")

    # WebSocket message senders
    async def _send_partial_transcript(self, text: str):
        """Send partial transcript event to browser"""
        try:
            await self.websocket.send_json({
                "event": "partial_transcript",
                "data": {
                    "text": text,
                    "session_id": str(self.session_id)
                }
            })
        except Exception as e:
            logger.error(f"‚ùå Error sending partial transcript: {e}")

    async def _send_final_transcript(self, text: str):
        """Send final transcript event to browser"""
        try:
            await self.websocket.send_json({
                "event": "final_transcript",
                "data": {
                    "text": text,
                    "session_id": str(self.session_id)
                }
            })
        except Exception as e:
            logger.error(f"‚ùå Error sending final transcript: {e}")

    async def _send_ai_response_chunk(self, text: str):
        """Send AI response chunk event to browser"""
        try:
            await self.websocket.send_json({
                "event": "ai_response_chunk",
                "data": {
                    "text": text,
                    "session_id": str(self.session_id)
                }
            })
        except Exception as e:
            logger.error(f"‚ùå Error sending AI response chunk: {e}")

    async def _send_ai_response_complete(self, text: str):
        """Send AI response complete event to browser"""
        try:
            await self.websocket.send_json({
                "event": "ai_response_complete",
                "data": {
                    "text": text,
                    "session_id": str(self.session_id)
                }
            })
        except Exception as e:
            logger.error(f"‚ùå Error sending AI response complete: {e}")

    async def _handle_tts_response(self, text: str):
        """
        Convert AI text response to audio and stream to browser

        Reuses Discord bot's proven Chatterbox integration pattern.
        Streams WAV audio directly to browser via WebSocket binary frames.

        Args:
            text: AI response text to synthesize
        """
        try:
            t_tts_start = time.time()
            logger.info(f"üîä Starting TTS synthesis for text: \"{text[:50]}...\"")

            # Check Chatterbox health
            chatterbox_url = os.getenv('CHATTERBOX_URL', 'http://chatterbox-tts:4123')
            if not await self._check_chatterbox_health(chatterbox_url):
                logger.warning("‚ö†Ô∏è Chatterbox unavailable, skipping TTS")
                await self._send_error("TTS service unavailable")
                return

            # Build TTS request (same pattern as Discord bot)
            tts_data = {
                'input': text,
                'response_format': 'wav',  # Browser-compatible
                'speed': 1.0,
                'voice': os.getenv('CHATTERBOX_VOICE_ID', 'default'),
                'streaming_strategy': 'word',
                'streaming_chunk_size': 100,
                'streaming_buffer_size': 3,
                'streaming_quality': 'fast'
            }

            # Send TTS start event
            await self._send_tts_start()

            # Stream audio from Chatterbox
            async with httpx.AsyncClient(timeout=60.0) as client:
                async with client.stream(
                    'POST',
                    f"{chatterbox_url}/v1/audio/speech/stream/upload",
                    data=tts_data,
                    headers={'Content-Type': 'application/x-www-form-urlencoded'}
                ) as response:
                    response.raise_for_status()

                    first_byte = True
                    total_bytes = 0

                    async for chunk in response.aiter_bytes(chunk_size=8192):
                        # Log first byte latency (critical UX metric)
                        if first_byte:
                            t_first_byte = time.time()
                            latency_s = t_first_byte - t_tts_start
                            logger.info(f"‚è±Ô∏è ‚≠ê LATENCY [TTS first byte]: {latency_s:.3f}s")
                            first_byte = False

                        # Stream chunk to browser as binary WebSocket frame
                        await self.websocket.send_bytes(chunk)
                        total_bytes += len(chunk)

            # Send completion event
            t_complete = time.time()
            total_latency_s = t_complete - t_tts_start
            logger.info(f"‚úÖ TTS complete ({total_bytes:,} bytes, {total_latency_s:.2f}s)")

            await self._send_tts_complete(total_latency_s)

        except httpx.HTTPStatusError as e:
            logger.error(f"‚ùå Chatterbox HTTP error: {e.response.status_code}", exc_info=True)
            await self._send_error(f"TTS HTTP error: {e.response.status_code}")
        except httpx.TimeoutException:
            logger.error("‚ùå Chatterbox TTS timeout", exc_info=True)
            await self._send_error("TTS request timed out")
        except Exception as e:
            logger.error(f"‚ùå TTS error: {e}", exc_info=True)
            await self._send_error(f"TTS failed: {str(e)}")

    async def _check_chatterbox_health(self, base_url: str) -> bool:
        """
        Check if Chatterbox TTS service is responding

        Args:
            base_url: Chatterbox base URL (e.g., http://chatterbox-tts:4123)

        Returns:
            True if healthy, False otherwise
        """
        try:
            # Strip /v1 if present
            health_url = base_url.rstrip('/v1')
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{health_url}/health")
                return response.status_code == 200
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chatterbox health check failed: {e}")
            return False

    async def _send_tts_start(self):
        """Send TTS start event to browser"""
        try:
            await self.websocket.send_json({
                "event": "tts_start",
                "data": {
                    "session_id": str(self.session_id)
                }
            })
        except Exception as e:
            logger.error(f"‚ùå Error sending TTS start: {e}")

    async def _send_tts_complete(self, duration_s: float):
        """Send TTS complete event to browser"""
        try:
            await self.websocket.send_json({
                "event": "tts_complete",
                "data": {
                    "session_id": str(self.session_id),
                    "duration_s": duration_s
                }
            })
        except Exception as e:
            logger.error(f"‚ùå Error sending TTS complete: {e}")

    async def _send_error(self, message: str):
        """Send error event to browser"""
        try:
            await self.websocket.send_json({
                "event": "error",
                "data": {
                    "message": message,
                    "session_id": str(self.session_id)
                }
            })
        except Exception as e:
            logger.error(f"‚ùå Error sending error message: {e}")

    async def _cleanup(self):
        """Clean up resources"""
        logger.info(f"üßπ Cleaning up WebRTC handler for user {self.user_id}")

        self.is_active = False

        # Cancel silence monitoring
        if self.silence_task and not self.silence_task.done():
            self.silence_task.cancel()

        # Close WhisperX connection
        if self.whisper_client:
            try:
                await self.whisper_client.close()
            except Exception as e:
                logger.error(f"‚ùå Error closing WhisperX: {e}")

        # Close WebSocket
        try:
            await self.websocket.close()
        except Exception as e:
            logger.debug(f"WebSocket already closed: {e}")

        logger.info(f"‚úÖ WebRTC handler cleanup complete")
